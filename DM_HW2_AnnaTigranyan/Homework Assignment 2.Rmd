---
title: "Homework Assignment 2"
author: "Genya Hakobyan"
date: '2022-04-12'
output: html_document
---


```{r echo=FALSE, message=FALSE, warning=FALSE, results="hide"}

library(dplyr)
library(tidyr)
library(ggplot2)
library(grid)
library(ltm)
library(caret)

```

# Task 1: Cleaning and Understanding Data and Logit

a. Load the file. Get rid of variables that are irrelevant for logistic regression analysis using the function select(). Also, skip variables which are absent in the description. Check whether the data types are correct, if not make appropriate corrections assigning labels to each level according to the data description.

b. For numeric variable replace missing values by column mean, for categorical variables, remove observations with missing values. How many variables and observations do you have before and now?

c. Check the relationship between each numeric variable and the presence of kidney disease. Save only two most important numeric variables using boxplots. Add mean values to boxplot. Comment on it.

d. Use glm() function to perform a logistic regression with Class as the response and one of numeric variables as the predictor by using the results of previous subtask. Use the summary() function to print the result. Is the coefficient of explanatory variable significant? Why?

e. Plot the relationship between response and predictor variables with sigmoid line.



# Solution 1

a.Load the file. Get rid of variables that are irrelevant for logistic regression analysis using the function select(). Also, skip variables which are absent in the description. Check whether the data types are correct, if not make appropriate corrections assigning labels to each level according to the data description.
```{r message=FALSE, warning=FALSE}
df = read.csv("biodata.csv")
head(df)

```

```{r message=FALSE, warning=FALSE}
df <- subset(df, select=-c(id, sc, pcv, pe))
```

```{r message=FALSE, warning=FALSE}
sapply(df, class)

```

```{r message=FALSE, warning=FALSE}

df$age <- as.integer(df$age)
df$bp <- as.numeric(df$bp)
df$bgr <- as.numeric(df$bgr)
df$sod <- as.numeric(df$sod)
df$pot <- as.numeric(df$pot)
df$hemo <- as.numeric(df$hemo)
df$wbcc <- as.numeric(df$wbcc)
df$rbcc <- as.numeric(df$rbcc)

df$su <- factor(df$su, levels = c(0:5), ordered=TRUE)

df$rbc <- factor(df$rbc, levels = c('abnormal', 'normal'), labels=c('abnormal', 'normal'))
df$ba <- factor(df$ba, levels = c('notpresent', 'present'), labels=c('notpresent', 'present'))
df$htn <- factor(df$htn, levels = c('no','yes'), labels=c('no','yes'))
df$dm <- factor(df$dm, levels = c('no','yes'), labels=c('no','yes'))
df$cad <- factor(df$cad, levels = c('no','yes'), labels=c('no','yes'))
df$appet <- factor(df$appet, levels = c('poor','good'), labels=c('poor','good'))
df$ane <- factor(df$ane, levels = c('no','yes'), labels=c('no','yes'))
df$class <- factor(df$class, levels = c(0, 1), labels=c('notckd','ckd'))
 
```

```{r message=FALSE, warning=FALSE, results="hide", echo=FALSE}
str(df)
```


<br><br>b.For numeric variable replace missing values by column mean, for categorical variables, remove observations with missing values. How many variables and observations do you have before and now?
```{r message=FALSE, warning=FALSE}
sum(is.na(df))

for(i in 1:ncol(df)) {
  if (is.integer(df[ , i]) || is.numeric(df[ , i])){
     df[ , i][is.na(df[ , i])] <- mean(df[ , i], na.rm = TRUE)
  }
}

sum(is.na(df))

df <- df %>% drop_na()

sum(is.na(df))

```

```{r message=FALSE, warning=FALSE}
num_of_column <- ncol(df)
cat("Number of variables in Data Frame :", num_of_column, "\n")

num_of_row <- nrow(df)
cat("Number of observations in Data Frame :", num_of_row)

```
Before we had 21 variables and 381 observations, now we have 17 variables and 322 observations.


<br><br>c.Check the relationship between each numeric variable and the presence of kidney disease. Save only two most important numeric variables using boxplots. Add mean values to boxplot. Comment on it.
```{r message=FALSE, warning=FALSE}
for(i in 1:ncol(df)) {
  if (is.integer(df[ , i]) || is.numeric(df[ , i])) {
      c <- biserial.cor(df[ , i], df$class)
      cat("Relationship between ", colnames(df)[i], " and  class  variable :", c, "\n")
  }
}

```
```{r message=FALSE, warning=FALSE}
ggplot(df, aes(x = class, y = hemo, color = class)) +     
  geom_boxplot(alpha = 0.7) +
  stat_summary(fun.y = mean, geom = "point", shape = 20, size = 5, color = "red", fill = "red")
```
<br>boxplot() helps to visualize the distribution of the data by quartile and detect the presence of outliers. 
In this case, the mean and the median are almost equal, as the distribution is symmetric.<br>
If hemoglobin level is high the risk of having chronic kidney disease is low.

```{r message=FALSE, warning=FALSE}
ggplot(df, aes(x=class, y=rbcc, color = class)) +
    geom_boxplot(alpha=0.7) +
    stat_summary(fun.y=mean, geom="point", shape=20, size=5, color="red", fill="red") 
  
```
<br>If red blood cells level is high the risk of having chronic kidney disease is low.<br>
We have outliers when 'chronic kidney disease' class is "ckd"


<br><br>d.Use glm() function to perform a logistic regression with Class as the response and one of numeric variables as the predictor by using the results of previous subtask. Use the summary() function to print the result. Is the coefficient of explanatory variable significant? Why?
```{r message=FALSE, warning=FALSE}
model = glm(class ~ hemo, data = df, family = binomial)
summary(model)

```
The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable. 
In the output shows the coefficients, their standard errors, the z-statistic and the associated p-values. "hemo" is statistically significant because  p-value is very insignificant.

<br><br>e.Plot the relationship between response and predictor variables with sigmoid line.
```{r message=FALSE, warning=FALSE, results="hide", echo=FALSE}
df$class <- as.integer(df$class)
df$class[df$class == 1] <- 0
df$class[df$class == 2] <- 1

```

```{r message=FALSE, warning=FALSE}
ggplot(df, aes(hemo, class)) + 
  geom_jitter(width=0, height=0.05, alpha = 0.5) +
  geom_smooth(method="glm", se=FALSE, method.args=list(family="binomial"))

```
```{r message=FALSE, warning=FALSE, results="hide", echo=FALSE}
df$class <- factor(df$class, levels = c(0, 1), labels=c('notckd','ckd'))

```





<br><br><br># Task 2: Simple Logistic Regression

Use the function glm() to perform logistic regression with Class as a response variable and one of the categorical variables as the predictor. Chose the significant one. Use the function summary() to print the results.

a. Interpret the coefficients of the explanatory variable in terms of absolute (rather than exponential values).

b. Evaluate the probability of/for the base value of explanatory variable. Comment on it.

c. What are Null deviance and residual deviance from the summary output?

d. Calculate the exponent of the β1 coefficient using only your data and functions addmargins() and table(). Comment on it.



# Solution 2

Use the function glm() to perform logistic regression with Class as a response variable and one of the categorical variables as the predictor. Chose the significant one. Use the function summary() to print the results.
```{r message=FALSE, warning=FALSE, results="hide"}
for(i in 1:(ncol(df)-1)) {
  if (is.factor(df[ , i])) {
      print(colnames(df)[i])
      gl_model = glm(class ~ df[ , i], data = df, family = binomial)
      print(summary(gl_model))
  }
}
      
```

```{r message=FALSE, warning=FALSE}

logitModel = glm(class ~ appet, data = df, family = binomial)
summary(logitModel)

```

<br><br>a.Interpret the coefficients of the explanatory variable in terms of absolute (rather than exponential values).
```{r message=FALSE, warning=FALSE}
coef(logitModel)

```
Coefficient  is a negative number:it means that  "appet" decreases the risk of having a chronic kidney disease.<br>
e^β1 = e^(-1.234494) = 0.290982 <br>
It means that the person "with good appet" has a 0.290982 times the odds of the  person "with poor appetpoor" of having chronic kidney disease.<br>
```{r}
exp(-1.234494)
```
<br><br>b.Evaluate the probability of/for the base value of explanatory variable. Comment on it.
```{r message=FALSE, warning=FALSE}
exp(coef(logitModel)) / (1 + exp(coef(logitModel)))

```
If person have "appetpoor"  then the  probability of having chronic kidney disease is  77%.<br>
The "appetgood" with 22% decreases the risk of having a chronic kidney disease.<br>

<br><br>c.What are Null deviance and residual deviance from the summary output?<br>

Null deviance: 440.36  on 321  degrees of freedom<br>
Residual deviance: 419.55  on 320  degrees of freedom<br>

The difference between Null deviance and Residual deviance tells us that the model is a good fit. Greater the difference better the model. It makes sense to consider the model good if that difference is big enough.


<br><br>d.Calculate the exponent of the β1 coefficient using only your data and functions addmargins() and table(). Comment on it.
```{r}
addmargins(table(df$appet,df$class))

```
From above table we can calculate:<br>
p(notckd/poor) = 20/87 = **0.23**<br>
P/(1-P) = 0.23/(1-0.23) = 0.29<br>
So we get **exp*(β1)** = **0.29**<br>
<br>






<br><br><br> 
# Task 3: Multiple Logistic Regression

a. Divide the data frame into Train and Test sets (70:30), such that the proportion of Clas for both training and testing sets will correspond to the distibution of Class in the whole data.2 Use the full data set to perform the model with Class as a dependent variable with the function stepAIC() to obtain the best model based on AIC criteria. Hide the output.

b. Remove all non-significant variables from the last model. Show only the best model in which all variables must be significant at least at the level 0.01. Use the summary() function to print the result.

c. Pick your own new observation and predict the response. Comment on it.

d. Now fit two logistic regression models using training data. Both models should be the result of previous subtask (a).

e. For the first model with only significant coefficients, predict the probability of the presence of chronic kidney disease for testing set. Compute the confusion matrix using table() function. Figure out the overall fraction of correct predictions, sensitivity, and specificity for the test data using only confusion matrix. Check your computations using the function confusionMatrix().

f. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.



# Solution 3

a.Divide the data frame into Train and Test sets (70:30), such that the proportion of Clas for both training and testing sets will correspond to the distibution of Class in the whole data.2 Use the full data set to perform the model with Class as a dependent variable with the function stepAIC() to obtain the best model based on AIC criteria. Hide the output.
```{r message=FALSE, warning=FALSE}
set.seed(3456)
trainIndex <- createDataPartition(df$class, p = 0.7, list = FALSE)
training <- df[trainIndex,]
testing <- df[-trainIndex,]

```
```{r message=FALSE, warning=FALSE}
fullModel <- glm(class ~ ., data = df, family = binomial)
summary(fullModel)

```
```{r message=FALSE, warning=FALSE}
step <- stepAIC(fullModel, trace = FALSE)
step

```


<br><br>b.Remove all non-significant variables from the last model. Show only the best model in which all variables must be significant at least at the level 0.01. Use the summary() function to print the result.
```{r message=FALSE, warning=FALSE}
bestModel <- glm(formula = class ~ rbc + bgr + hemo, family = binomial, data = df)
summary(bestModel)

```

<br><br>c.Pick your own new observation and predict the response. Comment on it.
```{r message=FALSE, warning=FALSE}
newdata <- df[5,]

prob <- bestModel %>% predict(newdata, type = "response")
predicted.class <- ifelse(prob > 0.5, "ckd", "notckd")
predicted.class

```
From above we can say that according to our model prediction (cutoff=50%) observed person have chronic kidney disease, which is true.


We can make a new data ourselves and see the result of the model.

```{r message=FALSE, warning=FALSE}
newdata <- data.frame(rbc = c("normal", "normal", "abnormal"), bgr = c(13.25, 10.2, 15.6), hemo = c(14.35, 9.0, 15.5))
probabilities <- bestModel %>% predict(newdata, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "ckd", "notckd")
probabilities
predicted.classes

```


<br><br>d.Now fit two logistic regression models using training data. Both models should be the result of previous subtask (a).
```{r message=FALSE, warning=FALSE}
glm_full <- glm(class ~ ., data = training, family = binomial)

```

```{r message=FALSE, warning=FALSE}
glm_best <- glm(class ~ rbc + bgr + hemo, data = training, family = binomial)

```




<br><br>e.For the first model with only significant coefficients, predict the probability of the presence of chronic kidney disease for testing set. Compute the confusion matrix using table() function. Figure out the overall fraction of correct predictions, sensitivity, and specificity for the test data using only confusion matrix. Check your computations using the function confusionMatrix().
```{r message=FALSE, warning=FALSE}
glm_probs_best <- predict(glm_best, newdata = testing, type = "response")
predicted_value <- ifelse(glm_probs_best > 0.5, "ckd", "notckd")

expected_value <- testing$class

conf_mat <- table(predicted_value, expected_value)
conf_mat
```

```{rv message=FALSE, warning=FALSE}
sensitivity <- conf_mat[2]/(conf_mat[2] + conf_mat[1])
specificity <- conf_mat[3]/(conf_mat[3] + conf_mat[4])
cat("Sensitivity :", sensitivity, "\n")
cat("specificity :", specificity)
  
```
```{r message=FALSE, warning=FALSE, results="hide", echo=FALSE}
expected_value <- factor(expected_value)
predicted_value <- factor(predicted_value)

```

```{r message=FALSE, warning=FALSE}
confusionMatrix(data = predicted_value, reference = expected_value)

```



<br><br>f.Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
<br>
From confusion matrix we can see that logistic regression model performed well: the accuracy is 94% and it means that the model makes good predictions in 94% cases.





<br><br><br>
# Questions

a.What is the difference between the ROC and the precision-recall curve (PRC)? When do we need to consider PRC? Why?

ROC Curves and Precision-Recall curves are two tools that help in the interpretation of probabilistic forecast for binary (two-class) classification predictive modeling problems.<br>
**->** ROC Curves summarize the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.<br>
**->** Precision-Recall curves summarize the trade-off between the true positive rate(recall) and the positive predictive value (Precision ) for a predictive model using different probability thresholds.<br>
**->** ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.<br>
Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).
The reason for this is that typically the large number of class 0 examples means we are less interested in the skill of the model at predicting class 0 correctly, e.g. high true negatives.



<br>b.Why Linear Probability Model is not preferable to Logistic Regression for models with binary response variable?

Linear Regression assumes that there is a linear relationship present between dependent and independent variables. It finds the best fitting line/plane that describes two or more variables.
Linear regression  does not deal with the fact that probability is restricted to the 0–1 range. As a result its estimates will differ if the range of X differs, even when the underlying process generating the data is the same.



<br>c.Is it possible to calculate R2 for Logistic Regression? Is it the same as for Linear Regression? Why?

Often when we fit a linear regression model, we use R-squared as a way to assess how well a model fits the data.
However, there is no such R-squared value for general linear models like logistic regression models.
Over the years,  were proposed different measures for logistic regression, with the objective usually that the measure inherits the properties of the familiar R squared from linear regression. One of them is McFadden’s R squared, it is one of the ‘pseudo R2’ value. <br>
We use the following formula to calculate McFadden’s R-Squared:<br>
McFadden’s R-Squared = 1 – (log likelihood_model / log likelihood_null)











